{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4d23016",
   "metadata": {},
   "source": [
    "# Kaggle: LLM Classification Finetuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e653a8",
   "metadata": {},
   "source": [
    "## 0. Environment and Dependencies\n",
    "\n",
    "Primary libraries used in this notebook:\n",
    "\n",
    "- `pandas`, `numpy`: data processing\n",
    "- `matplotlib`, `seaborn`: visualization\n",
    "- `scikit-learn`: dataset splitting and metrics (accuracy, log_loss)\n",
    "- `transformers`, `datasets`, `torch`: LLM fine-tuning (e.g., DistilBERT)\n",
    "\n",
    "Local environment (GPU setup):\n",
    "\n",
    "- PyTorch: 2.7.1+cu118\n",
    "- CUDA available: True\n",
    "- CUDA version: 11.8\n",
    "- GPU model: NVIDIA GeForce RTX 3050 Laptop GPU\n",
    "- Current device: 0\n",
    "\n",
    "Make sure all dependencies are installed before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dfec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU model: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Python executable: {sys.executable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18f14d5",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "- Train set: `Dataset/train.csv`\n",
    "- Test set: `Dataset/test.csv`\n",
    "\n",
    "Train set columns:\n",
    "\n",
    "- `id, model_a, model_b, prompt, response_a, response_b, winner_model_a, winner_model_b, winner_tie`\n",
    "\n",
    "Test set columns:\n",
    "\n",
    "- `id, prompt, response_a, response_b`\n",
    "\n",
    "Goal: given the prompt and two responses, predict which response users prefer (A / B / tie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a3425",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(\"Dataset\", \"train.csv\")\n",
    "test_path = os.path.join(\"Dataset\", \"test.csv\")\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "print(\"Train shape:\", df_train.shape)\n",
    "print(\"Test shape:\", df_test.shape)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afee0ef9",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Before training, visualize the dataset to understand its structure. Focus on:\n",
    "\n",
    "- Label distribution (winner)\n",
    "- Basic statistics and distributions of text lengths (prompt / response)\n",
    "- A few sample rows to understand the task format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148b9c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge winner columns into a single label\n",
    "def get_winner(row):\n",
    "    if row[\"winner_model_a\"] == 1:\n",
    "        return \"Model A\"\n",
    "    if row[\"winner_model_b\"] == 1:\n",
    "        return \"Model B\"\n",
    "    return \"Tie\"\n",
    "\n",
    "df_train[\"winner\"] = df_train.apply(get_winner, axis=1)\n",
    "\n",
    "df_train[\"winner\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5446e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"len_prompt\"] = df_train[\"prompt\"].astype(str).apply(len)\n",
    "df_train[\"len_resp_a\"] = df_train[\"response_a\"].astype(str).apply(len)\n",
    "df_train[\"len_resp_b\"] = df_train[\"response_b\"].astype(str).apply(len)\n",
    "\n",
    "q_prompt = df_train[\"len_prompt\"].quantile(0.95)\n",
    "q_resp_a = df_train[\"len_resp_a\"].quantile(0.95)\n",
    "q_resp_b = df_train[\"len_resp_b\"].quantile(0.95)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "sns.histplot(df_train[\"len_prompt\"], bins=50, ax=axes[0])\n",
    "axes[0].set_title(\"Prompt Length (0-95% quantile)\")\n",
    "axes[0].set_xlim(0, q_prompt)\n",
    "\n",
    "sns.histplot(df_train[\"len_resp_a\"], bins=50, ax=axes[1])\n",
    "axes[1].set_title(\"Response A Length (0-95% quantile)\")\n",
    "axes[1].set_xlim(0, q_resp_a)\n",
    "\n",
    "sns.histplot(df_train[\"len_resp_b\"], bins=50, ax=axes[2])\n",
    "axes[2].set_title(\"Response B Length (0-95% quantile)\")\n",
    "axes[2].set_xlim(0, q_resp_b)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab2f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[[\"prompt\", \"response_a\", \"response_b\", \"winner\"]].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54368ac4",
   "metadata": {},
   "source": [
    "## 3. Model Choice and Principles\n",
    "\n",
    "This is a **text multi-class classification** task: given `(prompt, response_a, response_b)`, predict which response is preferred (or tie).\n",
    "\n",
    "This notebook uses **DistilBERT** as the base model:\n",
    "\n",
    "- DistilBERT is a distilled, smaller BERT with fewer parameters and faster inference, while preserving most semantic capability.\n",
    "- Pretraining learns general language representations; fine-tuning maps them to the preference classification task.\n",
    "- Model card and paper:\n",
    "  - https://huggingface.co/distilbert-base-uncased\n",
    "  - https://arxiv.org/abs/1910.01108\n",
    "\n",
    "### Input Construction Strategy\n",
    "\n",
    "We concatenate `(prompt, response_a, response_b)` into a single sequence so the model can compare both responses within one context window.\n",
    "\n",
    "```text\n",
    "Prompt: <prompt> \\n Response A: <response_a> \\n Response B: <response_b>\n",
    "```\n",
    "\n",
    "Why this works:\n",
    "- Self-attention aligns key information across segments, learning prompt-response alignment and A/B differences.\n",
    "- The classifier reads a single global representation (e.g., [CLS] or pooled embedding), effectively comparing all three segments.\n",
    "- A fixed template (Prompt / Response A / Response B) makes the input structure explicit and stable.\n",
    "\n",
    "The output layer is a 3-class classifier:\n",
    "- Class 0: Model A wins\n",
    "- Class 1: Model B wins\n",
    "- Class 2: Tie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab63fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "label2id = {\"Model A\": 0, \"Model B\": 1, \"Tie\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "df_train[\"label\"] = df_train[\"winner\"].map(label2id)\n",
    "df_train[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642ca8a4",
   "metadata": {},
   "source": [
    "## 4. Metrics and Loss Function\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "- For multi-class classification, the standard choice is **cross-entropy loss**.\n",
    "- In `transformers`, `AutoModelForSequenceClassification` automatically applies cross-entropy when `labels` are provided.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "- **Accuracy**: correct predictions / total samples.\n",
    "- **Log Loss**: a Kaggle-standard metric that measures how close the predicted probability distribution is to the true labels (lower is better).\n",
    "\n",
    "In the Trainer, we compute both metrics via a custom `compute_metrics` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb6de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    texts = [\n",
    "        f\"Prompt: {p} \\n Response A: {a} \\n Response B: {b}\"\n",
    "        for p, a, b in zip(examples[\"prompt\"], examples[\"response_a\"], examples[\"response_b\"])\n",
    "    ]\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    if \"label\" in examples:\n",
    "        tokenized[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized\n",
    "\n",
    "# Split train/validation sets\n",
    "train_df, val_df = train_test_split(\n",
    "    df_train,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=df_train[\"label\"],\n",
    ")\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_dataset = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(df_test.reset_index(drop=True))\n",
    "\n",
    "train_enc = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "val_enc = val_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names,\n",
    ")\n",
    "test_enc = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset.column_names,\n",
    ")\n",
    "\n",
    "train_enc.set_format(\"torch\")\n",
    "val_enc.set_format(\"torch\")\n",
    "test_enc.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafeb82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    preds = probs.argmax(axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    try:\n",
    "        ll = log_loss(labels, probs)\n",
    "    except ValueError:\n",
    "        ll = float(\"nan\")\n",
    "    return {\"accuracy\": acc, \"log_loss\": ll}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a3c0cb",
   "metadata": {},
   "source": [
    "## 5. Model Building and Training\n",
    "\n",
    "We build a 3-class model with `AutoModelForSequenceClassification`:\n",
    "\n",
    "- `num_labels=3`\n",
    "- `id2label` / `label2id` map class IDs to readable labels.\n",
    "\n",
    "Key training parameters and what they do:\n",
    "- `learning_rate`: step size; too large causes instability, too small slows convergence. Typical range: 1e-5 to 5e-5.\n",
    "- `num_train_epochs`: number of full passes; higher can overfit. Monitor validation metrics as it increases.\n",
    "- `per_device_train_batch_size`: batch size per GPU; limited by VRAM. Use gradient accumulation to simulate larger batches.\n",
    "- `gradient_accumulation_steps`: accumulates gradients across steps; effective batch = batch_size Ã— accumulation_steps.\n",
    "- `weight_decay`: regularization to reduce overfitting; commonly 0.01.\n",
    "- `fp16`: mixed precision for faster training and lower memory usage on GPUs.\n",
    "- `eval_strategy` / `save_strategy`: evaluation and checkpointing cadence; must match when `load_best_model_at_end=True`.\n",
    "- `logging_steps`: log interval for tracking loss.\n",
    "- `save_total_limit`: limits checkpoint count to save disk space.\n",
    "- `warmup_ratio` or `warmup_steps`: warms up the learning rate for stability.\n",
    "\n",
    "Tuning tips:\n",
    "- Start with fewer epochs and a smaller batch to validate the pipeline, then scale up.\n",
    "- If validation loss rises, reduce epochs or increase regularization.\n",
    "- When VRAM is limited, use gradient accumulation instead of a larger batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524270e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ").to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "\n",
    "    # GPU parameters\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=True,\n",
    "\n",
    "    # training epochs\n",
    "    num_train_epochs=3,\n",
    "\n",
    "    # optimizer parameters\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "\n",
    "    # evaluation&save parameters\n",
    "    eval_strategy=\"steps\", # or \"epoch\"\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "\n",
    "    # logging&report parameters\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    # gradient_checkpointing=True # enable gradient checkpointing\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_enc,\n",
    "    eval_dataset=val_enc,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # callbacks=[transformers.EarlyStoppingCallback(early_stopping_patience=3)] # enable early stopping\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e45a1b",
   "metadata": {},
   "source": [
    "## 6. Validation Evaluation and Test Prediction\n",
    "\n",
    "1. Evaluate on the validation set and report accuracy and log_loss.\n",
    "2. Predict on the test set and generate `submission.csv` with:\n",
    "   - `id`\n",
    "   - `winner_model_a`, `winner_model_b`, `winner_tie` (predicted probabilities for each class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5984377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "train_metrics = history.loc[history[\"loss\"].notna(), [\"step\", \"loss\"]]\n",
    "eval_cols = [c for c in [\"eval_loss\", \"eval_accuracy\", \"eval_log_loss\", \"eval_runtime\", \"eval_samples_per_second\", \"eval_steps_per_second\"] if c in history.columns]\n",
    "eval_metrics = history.loc[history[\"eval_loss\"].notna(), [\"step\"] + eval_cols]\n",
    "\n",
    "train_metrics = train_metrics.rename(columns={\"loss\": \"Training Loss\"})\n",
    "eval_metrics = eval_metrics.rename(columns={\n",
    "    \"eval_loss\": \"Validation Loss\",\n",
    "    \"eval_accuracy\": \"Accuracy\",\n",
    "    \"eval_log_loss\": \"Log Loss\",\n",
    "    \"eval_runtime\": \"Runtime\",\n",
    "    \"eval_samples_per_second\": \"Samples Per Second\",\n",
    "    \"eval_steps_per_second\": \"Steps Per Second\",\n",
    "})\n",
    "\n",
    "metrics_table = eval_metrics.merge(train_metrics, on=\"step\", how=\"left\")\n",
    "metrics_table = metrics_table.rename(columns={\"step\": \"Step\"})\n",
    "metrics_table = metrics_table[[\"Step\", \"Training Loss\", \"Validation Loss\", \"Accuracy\", \"Log Loss\", \"Runtime\", \"Samples Per Second\", \"Steps Per Second\"]]\n",
    "\n",
    "metrics_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bf6ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(metrics_table[\"Step\"], metrics_table[\"Training Loss\"], label=\"Training Loss\")\n",
    "plt.plot(metrics_table[\"Step\"], metrics_table[\"Validation Loss\"], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(metrics_table[\"Step\"], metrics_table[\"Accuracy\"], label=\"Accuracy\")\n",
    "plt.plot(metrics_table[\"Step\"], metrics_table[\"Log Loss\"], label=\"Log Loss\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Metric\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\")\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e40041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test_enc)\n",
    "probs = torch.softmax(torch.tensor(predictions.predictions), dim=-1).numpy()\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": df_test[\"id\"],\n",
    "    \"winner_model_a\": probs[:, 0],\n",
    "    \"winner_model_b\": probs[:, 1],\n",
    "    \"winner_tie\": probs[:, 2],\n",
    "})\n",
    "\n",
    "submission_path = \"submission.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"Submission file saved to {submission_path}\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7507a9",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "- How to load and visualize Kaggle LLM Classification Finetuning `train.csv` / `test.csv` data\n",
    "- How to combine three text fields (prompt, response_a, response_b) into one model input\n",
    "- How to fine-tune a 3-class DistilBERT model with cross-entropy loss\n",
    "- How to evaluate on the validation set (accuracy, log_loss)\n",
    "- How to predict the test set and generate a submission file in the required format\n",
    "\n",
    "You can extend this by:\n",
    "\n",
    "- Trying a larger model or incorporating `model_a`/`model_b` as features\n",
    "- Improving the input construction (e.g., encode A/B separately then compare)\n",
    "- Adding richer visualizations and error analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
