{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Kaggle: LLM Classification Finetuning\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. Environment and Dependencies\n",
                "\n",
                "Primary libraries used in this notebook:\n",
                "\n",
                "- `pandas`, `numpy`: data processing\n",
                "- `matplotlib`, `seaborn`: visualization\n",
                "- `scikit-learn`: dataset splitting and metrics (accuracy, log_loss)\n",
                "- `transformers`, `datasets`, `torch`: LLM fine-tuning (e.g., DistilBERT)\n",
                "\n",
                "Local environment (My Laptop):\n",
                "\n",
                "- PyTorch: 2.7.1+cu118\n",
                "- CUDA available: True\n",
                "- CUDA version: 11.8\n",
                "- GPU model: NVIDIA GeForce RTX 3050 Laptop GPU\n",
                "- Current device: 0\n",
                "\n",
                "Make sure all dependencies are installed before running the notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2026-02-20T10:32:22.947462Z",
                    "iopub.status.busy": "2026-02-20T10:32:22.947128Z",
                    "iopub.status.idle": "2026-02-20T10:32:49.013805Z",
                    "shell.execute_reply": "2026-02-20T10:32:49.012812Z",
                    "shell.execute_reply.started": "2026-02-20T10:32:22.947424Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import accuracy_score, log_loss\n",
                "\n",
                "import torch\n",
                "from datasets import Dataset\n",
                "import transformers\n",
                "from transformers import (\n",
                "    AutoTokenizer,\n",
                "    AutoModelForSequenceClassification,\n",
                "    Trainer,\n",
                "    TrainingArguments,\n",
                "    DataCollatorWithPadding,\n",
                ")\n",
                "\n",
                "sns.set_theme(style=\"whitegrid\")\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "print(f\"CUDA version: {torch.version.cuda}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU model: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
                "print(f\"Transformers version: {transformers.__version__}\")\n",
                "print(f\"Python executable: {sys.executable}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading\n",
                "\n",
                "- Train set: `Dataset/train.csv`\n",
                "- Test set: `Dataset/test.csv`\n",
                "\n",
                "Train set columns:\n",
                "\n",
                "- `id, model_a, model_b, prompt, response_a, response_b, winner_model_a, winner_model_b, winner_tie`\n",
                "\n",
                "Test set columns:\n",
                "\n",
                "- `id, prompt, response_a, response_b`\n",
                "\n",
                "Goal: given the prompt and two responses, predict which response users prefer (A / B / tie)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2026-02-20T10:33:12.384975Z",
                    "iopub.status.busy": "2026-02-20T10:33:12.384088Z",
                    "iopub.status.idle": "2026-02-20T10:33:15.988249Z",
                    "shell.execute_reply": "2026-02-20T10:33:15.987478Z",
                    "shell.execute_reply.started": "2026-02-20T10:33:12.384940Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "data_root = \"/kaggle/input/competitions/llm-classification-finetuning\"\n",
                "train_path = os.path.join(data_root, \"train.csv\")\n",
                "test_path = os.path.join(data_root, \"test.csv\")\n",
                "\n",
                "df_train = pd.read_csv(train_path)\n",
                "df_test = pd.read_csv(test_path)\n",
                "\n",
                "print(\"Train shape:\", df_train.shape)\n",
                "print(\"Test shape:\", df_test.shape)\n",
                "\n",
                "df_train.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Exploratory Data Analysis (EDA)\n",
                "\n",
                "Before training, visualize the dataset to understand its structure. Focus on:\n",
                "\n",
                "- Label distribution (winner)\n",
                "- Basic statistics and distributions of text lengths (prompt / response)\n",
                "- A few sample rows to understand the task format"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2026-02-20T10:33:19.495448Z",
                    "iopub.status.busy": "2026-02-20T10:33:19.495127Z",
                    "iopub.status.idle": "2026-02-20T10:33:19.864151Z",
                    "shell.execute_reply": "2026-02-20T10:33:19.863013Z",
                    "shell.execute_reply.started": "2026-02-20T10:33:19.495421Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "# Merge winner columns into a single label\n",
                "def get_winner(row):\n",
                "    if row[\"winner_model_a\"] == 1:\n",
                "        return \"Model A\"\n",
                "    if row[\"winner_model_b\"] == 1:\n",
                "        return \"Model B\"\n",
                "    return \"Tie\"\n",
                "\n",
                "df_train[\"winner\"] = df_train.apply(get_winner, axis=1)\n",
                "\n",
                "df_train[\"winner\"].value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2026-02-20T10:33:21.744779Z",
                    "iopub.status.busy": "2026-02-20T10:33:21.744170Z",
                    "iopub.status.idle": "2026-02-20T10:33:22.626206Z",
                    "shell.execute_reply": "2026-02-20T10:33:22.625189Z",
                    "shell.execute_reply.started": "2026-02-20T10:33:21.744746Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "df_train[\"len_prompt\"] = df_train[\"prompt\"].astype(str).apply(len)\n",
                "df_train[\"len_resp_a\"] = df_train[\"response_a\"].astype(str).apply(len)\n",
                "df_train[\"len_resp_b\"] = df_train[\"response_b\"].astype(str).apply(len)\n",
                "\n",
                "q_prompt = df_train[\"len_prompt\"].quantile(0.95)\n",
                "q_resp_a = df_train[\"len_resp_a\"].quantile(0.95)\n",
                "q_resp_b = df_train[\"len_resp_b\"].quantile(0.95)\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "sns.histplot(df_train[\"len_prompt\"], bins=50, ax=axes[0])\n",
                "axes[0].set_title(\"Prompt Length (0-95% quantile)\")\n",
                "axes[0].set_xlim(0, q_prompt)\n",
                "\n",
                "sns.histplot(df_train[\"len_resp_a\"], bins=50, ax=axes[1])\n",
                "axes[1].set_title(\"Response A Length (0-95% quantile)\")\n",
                "axes[1].set_xlim(0, q_resp_a)\n",
                "\n",
                "sns.histplot(df_train[\"len_resp_b\"], bins=50, ax=axes[2])\n",
                "axes[2].set_title(\"Response B Length (0-95% quantile)\")\n",
                "axes[2].set_xlim(0, q_resp_b)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2026-02-20T10:33:25.211792Z",
                    "iopub.status.busy": "2026-02-20T10:33:25.211031Z",
                    "iopub.status.idle": "2026-02-20T10:33:25.230441Z",
                    "shell.execute_reply": "2026-02-20T10:33:25.229443Z",
                    "shell.execute_reply.started": "2026-02-20T10:33:25.211757Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "df_train[[\"prompt\", \"response_a\", \"response_b\", \"winner\"]].head(3)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Choice and Principles\n",
                "\n",
                "This is a **text multi-class classification** task: given `(prompt, response_a, response_b)`, predict which response is preferred (or tie).\n",
                "\n",
                "This notebook uses **DistilBERT** as the base model:\n",
                "\n",
                "- DistilBERT is a distilled, smaller BERT with fewer parameters and faster inference, while preserving most semantic capability.\n",
                "- Pretraining learns general language representations; fine-tuning maps them to the preference classification task.\n",
                "\n",
                "### Input Construction Strategy\n",
                "\n",
                "We concatenate `(prompt, response_a, response_b)` into a single sequence so the model can compare both responses within one context window.\n",
                "\n",
                "```text\n",
                "Prompt: <prompt> \\n Response A: <response_a> \\n Response B: <response_b>\n",
                "```\n",
                "\n",
                "Why this works:\n",
                "- Self-attention aligns key information across segments, learning prompt-response alignment and A/B differences.\n",
                "- The classifier reads a single global representation (e.g., [CLS] or pooled embedding), effectively comparing all three segments.\n",
                "- A fixed template (Prompt / Response A / Response B) makes the input structure explicit and stable.\n",
                "\n",
                "The output layer is a 3-class classifier:\n",
                "- Class 0: Model A wins\n",
                "- Class 1: Model B wins\n",
                "- Class 2: Tie"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2026-02-20T10:33:26.994068Z",
                    "iopub.status.busy": "2026-02-20T10:33:26.993762Z",
                    "iopub.status.idle": "2026-02-20T10:33:27.168743Z",
                    "shell.execute_reply": "2026-02-20T10:33:27.167753Z",
                    "shell.execute_reply.started": "2026-02-20T10:33:26.994043Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "HF_LOCAL_MODEL_DIR = \"/kaggle/input/models/boriszhangyyy/distilbert-finetuned-llm-pref/pytorch/default/1/results/checkpoint-19401\"\n",
                "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
                "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
                "MODEL_NAME = HF_LOCAL_MODEL_DIR if os.path.isdir(HF_LOCAL_MODEL_DIR) else None\n",
                "if MODEL_NAME is None:\n",
                "    raise FileNotFoundError(\"No local model directory found at the Kaggle path.\")\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, local_files_only=True)\n",
                "\n",
                "label2id = {\"Model A\": 0, \"Model B\": 1, \"Tie\": 2}\n",
                "id2label = {v: k for k, v in label2id.items()}\n",
                "\n",
                "df_train[\"label\"] = df_train[\"winner\"].map(label2id)\n",
                "df_train[\"label\"].value_counts()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Metrics and Loss Function\n",
                "\n",
                "### Loss Function\n",
                "\n",
                "- For multi-class classification, the standard choice is **cross-entropy loss**.\n",
                "- In `transformers`, `AutoModelForSequenceClassification` automatically applies cross-entropy when `labels` are provided.\n",
                "\n",
                "### Metrics\n",
                "\n",
                "- **Accuracy**: correct predictions / total samples.\n",
                "- **Log Loss**: a Kaggle-standard metric that measures how close the predicted probability distribution is to the true labels (lower is better).\n",
                "\n",
                "In the Trainer, we compute both metrics via a custom `compute_metrics` function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2026-02-20T10:33:29.270807Z",
                    "iopub.status.busy": "2026-02-20T10:33:29.270372Z",
                    "iopub.status.idle": "2026-02-20T10:35:04.789450Z",
                    "shell.execute_reply": "2026-02-20T10:35:04.788304Z",
                    "shell.execute_reply.started": "2026-02-20T10:33:29.270777Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "def preprocess_function(examples):\n",
                "    texts = [\n",
                "        f\"Prompt: {p} \\n Response A: {a} \\n Response B: {b}\"\n",
                "        for p, a, b in zip(examples[\"prompt\"], examples[\"response_a\"], examples[\"response_b\"])\n",
                "    ]\n",
                "    tokenized = tokenizer(\n",
                "        texts,\n",
                "        padding=\"max_length\",\n",
                "        truncation=True,\n",
                "        max_length=512,\n",
                "    )\n",
                "    if \"label\" in examples:\n",
                "        tokenized[\"labels\"] = examples[\"label\"]\n",
                "    return tokenized\n",
                "\n",
                "test_dataset = Dataset.from_pandas(df_test.reset_index(drop=True))\n",
                "\n",
                "test_enc = test_dataset.map(\n",
                "    preprocess_function,\n",
                "    batched=True,\n",
                "    remove_columns=test_dataset.column_names,\n",
                ")\n",
                "\n",
                "test_enc.set_format(\"torch\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2026-02-20T10:35:04.791561Z",
                    "iopub.status.busy": "2026-02-20T10:35:04.791182Z",
                    "iopub.status.idle": "2026-02-20T10:35:04.797406Z",
                    "shell.execute_reply": "2026-02-20T10:35:04.796414Z",
                    "shell.execute_reply.started": "2026-02-20T10:35:04.791533Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "def compute_metrics(eval_pred):\n",
                "    logits, labels = eval_pred\n",
                "    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
                "    preds = probs.argmax(axis=-1)\n",
                "    acc = accuracy_score(labels, preds)\n",
                "    try:\n",
                "        ll = log_loss(labels, probs)\n",
                "    except ValueError:\n",
                "        ll = float(\"nan\")\n",
                "    return {\"accuracy\": acc, \"log_loss\": ll}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model Building and Training\n",
                "\n",
                "We build a 3-class model with `AutoModelForSequenceClassification`:\n",
                "\n",
                "- `num_labels=3`\n",
                "- `id2label` / `label2id` map class IDs to readable labels.\n",
                "\n",
                "Key training parameters and what they do:\n",
                "- `learning_rate`: step size; too large causes instability, too small slows convergence. Typical range: 1e-5 to 5e-5.\n",
                "- `num_train_epochs`: number of full passes; higher can overfit. Monitor validation metrics as it increases.\n",
                "- `per_device_train_batch_size`: batch size per GPU; limited by VRAM. Use gradient accumulation to simulate larger batches.\n",
                "- `gradient_accumulation_steps`: accumulates gradients across steps; effective batch = batch_size Ã— accumulation_steps.\n",
                "- `weight_decay`: regularization to reduce overfitting; commonly 0.01.\n",
                "- `fp16`: mixed precision for faster training and lower memory usage on GPUs.\n",
                "- `eval_strategy` / `save_strategy`: evaluation and checkpointing cadence; must match when `load_best_model_at_end=True`.\n",
                "- `logging_steps`: log interval for tracking loss.\n",
                "- `save_total_limit`: limits checkpoint count to save disk space.\n",
                "- `warmup_ratio` or `warmup_steps`: warms up the learning rate for stability.\n",
                "\n",
                "Tuning tips:\n",
                "- Start with fewer epochs and a smaller batch to validate the pipeline, then scale up.\n",
                "- If validation loss rises, reduce epochs or increase regularization.\n",
                "- When VRAM is limited, use gradient accumulation instead of a larger batch."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "execution_failed": "2026-02-20T10:38:31.095Z",
                    "iopub.execute_input": "2026-02-20T10:35:04.799005Z",
                    "iopub.status.busy": "2026-02-20T10:35:04.798680Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "model = AutoModelForSequenceClassification.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    num_labels=3,\n",
                "    id2label=id2label,\n",
                "    label2id=label2id,\n",
                "    local_files_only=True,\n",
                ").to(device)\n",
                "\n",
                "inference_args = TrainingArguments(\n",
                "    output_dir=\"./results\",\n",
                "    per_device_eval_batch_size=16,\n",
                "    report_to=\"none\",\n",
                ")\n",
                "\n",
                "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=inference_args,\n",
                "    data_collator=data_collator,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Validation Evaluation and Test Prediction\n",
                "\n",
                "1. Evaluate on the validation set and report accuracy and log_loss.\n",
                "2. Predict on the test set and generate `submission.csv` with:\n",
                "   - `id`\n",
                "   - `winner_model_a`, `winner_model_b`, `winner_tie` (predicted probabilities for each class)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "execution_failed": "2026-02-20T10:38:31.096Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "print(\"Skip evaluation in inference-only mode\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "execution_failed": "2026-02-20T10:38:31.096Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "predictions = trainer.predict(test_enc)\n",
                "probs = torch.softmax(torch.tensor(predictions.predictions), dim=-1).numpy()\n",
                "\n",
                "submission = pd.DataFrame({\n",
                "    \"id\": df_test[\"id\"],\n",
                "    \"winner_model_a\": probs[:, 0],\n",
                "    \"winner_model_b\": probs[:, 1],\n",
                "    \"winner_tie\": probs[:, 2],\n",
                "})\n",
                "\n",
                "submission_path = \"/kaggle/working/submission.csv\"\n",
                "submission.to_csv(submission_path, index=False)\n",
                "print(f\"Submission file saved to {submission_path}\")\n",
                "submission.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Summary\n",
                "\n",
                "This notebook demonstrates:\n",
                "\n",
                "- How to load and visualize Kaggle LLM Classification Finetuning `train.csv` / `test.csv` data\n",
                "- How to combine three text fields (prompt, response_a, response_b) into one model input\n",
                "- How to fine-tune a 3-class DistilBERT model with cross-entropy loss\n",
                "- How to evaluate on the validation set (accuracy, log_loss)\n",
                "- How to predict the test set and generate a submission file in the required format\n",
                "\n",
                "You can extend this by:\n",
                "\n",
                "- Trying a larger model or incorporating `model_a`/`model_b` as features\n",
                "- Improving the input construction (e.g., encode A/B separately then compare)\n",
                "- Adding richer visualizations and error analysis"
            ]
        }
    ],
    "metadata": {
        "kaggle": {
            "accelerator": "none",
            "dataSources": [
                {
                    "databundleVersionId": 9809560,
                    "sourceId": 86518,
                    "sourceType": "competition"
                },
                {
                    "databundleVersionId": 15762763,
                    "modelId": 590122,
                    "modelInstanceId": 577793,
                    "sourceId": 756504,
                    "sourceType": "modelInstanceVersion"
                }
            ],
            "dockerImageVersionId": 31286,
            "isGpuEnabled": false,
            "isInternetEnabled": false,
            "language": "python",
            "sourceType": "notebook"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
