{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"competition","sourceId":86518,"databundleVersionId":9809560},{"sourceType":"modelInstanceVersion","sourceId":756504,"databundleVersionId":15762763,"modelInstanceId":577793,"modelId":590122}],"dockerImageVersionId":31286,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Kaggle: LLM Classification Finetuning\n","metadata":{}},{"cell_type":"markdown","source":"## 0. Environment and Dependencies\n\nPrimary libraries used in this notebook:\n\n- `pandas`, `numpy`: data processing\n- `matplotlib`, `seaborn`: visualization\n- `scikit-learn`: dataset splitting and metrics (accuracy, log_loss)\n- `transformers`, `datasets`, `torch`: LLM fine-tuning (e.g., DistilBERT)\n\nLocal environment (My Laptop):\n\n- PyTorch: 2.7.1+cu118\n- CUDA available: True\n- CUDA version: 11.8\n- GPU model: NVIDIA GeForce RTX 3050 Laptop GPU\n- Current device: 0\n\nMake sure all dependencies are installed before running the notebook.","metadata":{}},{"cell_type":"code","source":"import sys\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss\n\nimport torch\nfrom datasets import Dataset\nimport transformers\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding,\n)\n\nsns.set_theme(style=\"whitegrid\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\nif torch.cuda.is_available():\n    print(f\"GPU model: {torch.cuda.get_device_name(0)}\")\n    print(f\"Current device: {torch.cuda.current_device()}\")\nprint(f\"Transformers version: {transformers.__version__}\")\nprint(f\"Python executable: {sys.executable}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T10:32:22.947128Z","iopub.execute_input":"2026-02-20T10:32:22.947462Z","iopub.status.idle":"2026-02-20T10:32:49.013805Z","shell.execute_reply.started":"2026-02-20T10:32:22.947424Z","shell.execute_reply":"2026-02-20T10:32:49.012812Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Data Loading\n\n- Train set: `Dataset/train.csv`\n- Test set: `Dataset/test.csv`\n\nTrain set columns:\n\n- `id, model_a, model_b, prompt, response_a, response_b, winner_model_a, winner_model_b, winner_tie`\n\nTest set columns:\n\n- `id, prompt, response_a, response_b`\n\nGoal: given the prompt and two responses, predict which response users prefer (A / B / tie).","metadata":{}},{"cell_type":"code","source":"data_root = \"/kaggle/input/llm-classification-finetuning\"\ntrain_path = os.path.join(data_root, \"train.csv\")\ntest_path = os.path.join(data_root, \"test.csv\")\n\ndf_train = pd.read_csv(train_path)\ndf_test = pd.read_csv(test_path)\n\nprint(\"Train shape:\", df_train.shape)\nprint(\"Test shape:\", df_test.shape)\n\ndf_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T10:33:12.384088Z","iopub.execute_input":"2026-02-20T10:33:12.384975Z","iopub.status.idle":"2026-02-20T10:33:15.988249Z","shell.execute_reply.started":"2026-02-20T10:33:12.384940Z","shell.execute_reply":"2026-02-20T10:33:15.987478Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Exploratory Data Analysis (EDA)\n\nBefore training, visualize the dataset to understand its structure. Focus on:\n\n- Label distribution (winner)\n- Basic statistics and distributions of text lengths (prompt / response)\n- A few sample rows to understand the task format","metadata":{}},{"cell_type":"code","source":"# Merge winner columns into a single label\ndef get_winner(row):\n    if row[\"winner_model_a\"] == 1:\n        return \"Model A\"\n    if row[\"winner_model_b\"] == 1:\n        return \"Model B\"\n    return \"Tie\"\n\ndf_train[\"winner\"] = df_train.apply(get_winner, axis=1)\n\ndf_train[\"winner\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T10:33:19.495127Z","iopub.execute_input":"2026-02-20T10:33:19.495448Z","iopub.status.idle":"2026-02-20T10:33:19.864151Z","shell.execute_reply.started":"2026-02-20T10:33:19.495421Z","shell.execute_reply":"2026-02-20T10:33:19.863013Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train[\"len_prompt\"] = df_train[\"prompt\"].astype(str).apply(len)\ndf_train[\"len_resp_a\"] = df_train[\"response_a\"].astype(str).apply(len)\ndf_train[\"len_resp_b\"] = df_train[\"response_b\"].astype(str).apply(len)\n\nq_prompt = df_train[\"len_prompt\"].quantile(0.95)\nq_resp_a = df_train[\"len_resp_a\"].quantile(0.95)\nq_resp_b = df_train[\"len_resp_b\"].quantile(0.95)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\nsns.histplot(df_train[\"len_prompt\"], bins=50, ax=axes[0])\naxes[0].set_title(\"Prompt Length (0-95% quantile)\")\naxes[0].set_xlim(0, q_prompt)\n\nsns.histplot(df_train[\"len_resp_a\"], bins=50, ax=axes[1])\naxes[1].set_title(\"Response A Length (0-95% quantile)\")\naxes[1].set_xlim(0, q_resp_a)\n\nsns.histplot(df_train[\"len_resp_b\"], bins=50, ax=axes[2])\naxes[2].set_title(\"Response B Length (0-95% quantile)\")\naxes[2].set_xlim(0, q_resp_b)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T10:33:21.744170Z","iopub.execute_input":"2026-02-20T10:33:21.744779Z","iopub.status.idle":"2026-02-20T10:33:22.626206Z","shell.execute_reply.started":"2026-02-20T10:33:21.744746Z","shell.execute_reply":"2026-02-20T10:33:22.625189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train[[\"prompt\", \"response_a\", \"response_b\", \"winner\"]].head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T10:33:25.211031Z","iopub.execute_input":"2026-02-20T10:33:25.211792Z","iopub.status.idle":"2026-02-20T10:33:25.230441Z","shell.execute_reply.started":"2026-02-20T10:33:25.211757Z","shell.execute_reply":"2026-02-20T10:33:25.229443Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Model Choice and Principles\n\nThis is a **text multi-class classification** task: given `(prompt, response_a, response_b)`, predict which response is preferred (or tie).\n\nThis notebook uses **DistilBERT** as the base model:\n\n- DistilBERT is a distilled, smaller BERT with fewer parameters and faster inference, while preserving most semantic capability.\n- Pretraining learns general language representations; fine-tuning maps them to the preference classification task.\n\n### Input Construction Strategy\n\nWe concatenate `(prompt, response_a, response_b)` into a single sequence so the model can compare both responses within one context window.\n\n```text\nPrompt: <prompt> \\n Response A: <response_a> \\n Response B: <response_b>\n```\n\nWhy this works:\n- Self-attention aligns key information across segments, learning prompt-response alignment and A/B differences.\n- The classifier reads a single global representation (e.g., [CLS] or pooled embedding), effectively comparing all three segments.\n- A fixed template (Prompt / Response A / Response B) makes the input structure explicit and stable.\n\nThe output layer is a 3-class classifier:\n- Class 0: Model A wins\n- Class 1: Model B wins\n- Class 2: Tie","metadata":{}},{"cell_type":"code","source":"HF_LOCAL_MODEL_DIR = \"/kaggle/input/models/boriszhangyyy/distilbert-finetuned-llm-pref/pytorch/default/1/results/checkpoint-19401\"\nos.environ[\"HF_HUB_OFFLINE\"] = \"1\"\nos.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\nMODEL_NAME = HF_LOCAL_MODEL_DIR if os.path.isdir(HF_LOCAL_MODEL_DIR) else None\nif MODEL_NAME is None:\n    raise FileNotFoundError(\"No local model directory found at the Kaggle path.\")\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, local_files_only=True)\n\nlabel2id = {\"Model A\": 0, \"Model B\": 1, \"Tie\": 2}\nid2label = {v: k for k, v in label2id.items()}\n\ndf_train[\"label\"] = df_train[\"winner\"].map(label2id)\ndf_train[\"label\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T10:33:26.993762Z","iopub.execute_input":"2026-02-20T10:33:26.994068Z","iopub.status.idle":"2026-02-20T10:33:27.168743Z","shell.execute_reply.started":"2026-02-20T10:33:26.994043Z","shell.execute_reply":"2026-02-20T10:33:27.167753Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Metrics and Loss Function\n\n### Loss Function\n\n- For multi-class classification, the standard choice is **cross-entropy loss**.\n- In `transformers`, `AutoModelForSequenceClassification` automatically applies cross-entropy when `labels` are provided.\n\n### Metrics\n\n- **Accuracy**: correct predictions / total samples.\n- **Log Loss**: a Kaggle-standard metric that measures how close the predicted probability distribution is to the true labels (lower is better).\n\nIn the Trainer, we compute both metrics via a custom `compute_metrics` function.","metadata":{}},{"cell_type":"code","source":"def preprocess_function(examples):\n    texts = [\n        f\"Prompt: {p} \\n Response A: {a} \\n Response B: {b}\"\n        for p, a, b in zip(examples[\"prompt\"], examples[\"response_a\"], examples[\"response_b\"])\n    ]\n    tokenized = tokenizer(\n        texts,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512,\n    )\n    if \"label\" in examples:\n        tokenized[\"labels\"] = examples[\"label\"]\n    return tokenized\n\n# Split train/validation sets\ntrain_df, val_df = train_test_split(\n    df_train,\n    test_size=0.1,\n    random_state=42,\n    stratify=df_train[\"label\"],\n)\n\ntrain_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\nval_dataset = Dataset.from_pandas(val_df.reset_index(drop=True))\ntest_dataset = Dataset.from_pandas(df_test.reset_index(drop=True))\n\ntrain_enc = train_dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=train_dataset.column_names,\n)\nval_enc = val_dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=val_dataset.column_names,\n)\ntest_enc = test_dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=test_dataset.column_names,\n)\n\ntrain_enc.set_format(\"torch\")\nval_enc.set_format(\"torch\")\ntest_enc.set_format(\"torch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T10:33:29.270372Z","iopub.execute_input":"2026-02-20T10:33:29.270807Z","iopub.status.idle":"2026-02-20T10:35:04.789450Z","shell.execute_reply.started":"2026-02-20T10:33:29.270777Z","shell.execute_reply":"2026-02-20T10:35:04.788304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n    preds = probs.argmax(axis=-1)\n    acc = accuracy_score(labels, preds)\n    try:\n        ll = log_loss(labels, probs)\n    except ValueError:\n        ll = float(\"nan\")\n    return {\"accuracy\": acc, \"log_loss\": ll}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T10:35:04.791182Z","iopub.execute_input":"2026-02-20T10:35:04.791561Z","iopub.status.idle":"2026-02-20T10:35:04.797406Z","shell.execute_reply.started":"2026-02-20T10:35:04.791533Z","shell.execute_reply":"2026-02-20T10:35:04.796414Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Model Building and Training\n\nWe build a 3-class model with `AutoModelForSequenceClassification`:\n\n- `num_labels=3`\n- `id2label` / `label2id` map class IDs to readable labels.\n\nKey training parameters and what they do:\n- `learning_rate`: step size; too large causes instability, too small slows convergence. Typical range: 1e-5 to 5e-5.\n- `num_train_epochs`: number of full passes; higher can overfit. Monitor validation metrics as it increases.\n- `per_device_train_batch_size`: batch size per GPU; limited by VRAM. Use gradient accumulation to simulate larger batches.\n- `gradient_accumulation_steps`: accumulates gradients across steps; effective batch = batch_size Ã— accumulation_steps.\n- `weight_decay`: regularization to reduce overfitting; commonly 0.01.\n- `fp16`: mixed precision for faster training and lower memory usage on GPUs.\n- `eval_strategy` / `save_strategy`: evaluation and checkpointing cadence; must match when `load_best_model_at_end=True`.\n- `logging_steps`: log interval for tracking loss.\n- `save_total_limit`: limits checkpoint count to save disk space.\n- `warmup_ratio` or `warmup_steps`: warms up the learning rate for stability.\n\nTuning tips:\n- Start with fewer epochs and a smaller batch to validate the pipeline, then scale up.\n- If validation loss rises, reduce epochs or increase regularization.\n- When VRAM is limited, use gradient accumulation instead of a larger batch.","metadata":{}},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=3,\n    id2label=id2label,\n    label2id=label2id,\n    local_files_only=True,\n).to(device)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    learning_rate=2e-5,\n\n    # GPU parameters\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    gradient_accumulation_steps=1,\n    fp16=True,\n\n    # training epochs\n    num_train_epochs=3,\n\n    # optimizer parameters\n    weight_decay=0.01,\n    warmup_ratio=0.1,\n\n    # evaluation&save parameters\n    eval_strategy=\"steps\", # or \"epoch\"\n    eval_steps=100,\n    save_strategy=\"steps\",\n    save_steps=200,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n\n    # logging&report parameters\n    logging_steps=50,\n    report_to=\"none\",\n    # gradient_checkpointing=True # enable gradient checkpointing\n)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_enc,\n    eval_dataset=val_enc,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    # callbacks=[transformers.EarlyStoppingCallback(early_stopping_patience=3)] # enable early stopping\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T10:35:04.798680Z","iopub.execute_input":"2026-02-20T10:35:04.799005Z","execution_failed":"2026-02-20T10:38:31.095Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Validation Evaluation and Test Prediction\n\n1. Evaluate on the validation set and report accuracy and log_loss.\n2. Predict on the test set and generate `submission.csv` with:\n   - `id`\n   - `winner_model_a`, `winner_model_b`, `winner_tie` (predicted probabilities for each class).","metadata":{}},{"cell_type":"code","source":"eval_results = trainer.evaluate()\nprint(\"Evaluation results:\")\nprint(eval_results)","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-20T10:38:31.096Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = trainer.predict(test_enc)\nprobs = torch.softmax(torch.tensor(predictions.predictions), dim=-1).numpy()\n\nsubmission = pd.DataFrame({\n    \"id\": df_test[\"id\"],\n    \"winner_model_a\": probs[:, 0],\n    \"winner_model_b\": probs[:, 1],\n    \"winner_tie\": probs[:, 2],\n})\n\nsubmission_path = \"/kaggle/working/submission.csv\"\nsubmission.to_csv(submission_path, index=False)\nprint(f\"Submission file saved to {submission_path}\")\nsubmission.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-20T10:38:31.096Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Summary\n\nThis notebook demonstrates:\n\n- How to load and visualize Kaggle LLM Classification Finetuning `train.csv` / `test.csv` data\n- How to combine three text fields (prompt, response_a, response_b) into one model input\n- How to fine-tune a 3-class DistilBERT model with cross-entropy loss\n- How to evaluate on the validation set (accuracy, log_loss)\n- How to predict the test set and generate a submission file in the required format\n\nYou can extend this by:\n\n- Trying a larger model or incorporating `model_a`/`model_b` as features\n- Improving the input construction (e.g., encode A/B separately then compare)\n- Adding richer visualizations and error analysis","metadata":{}}]}