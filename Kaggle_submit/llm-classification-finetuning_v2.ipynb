{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4d23016",
   "metadata": {},
   "source": [
    "# Kaggle: LLM Classification Finetuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e653a8",
   "metadata": {},
   "source": [
    "## 0. Environment and Dependencies\n",
    "\n",
    "Primary libraries used in this notebook:\n",
    "\n",
    "- `pandas`, `numpy`: data processing\n",
    "- `matplotlib`, `seaborn`: visualization\n",
    "- `scikit-learn`: dataset splitting and metrics (accuracy, log_loss)\n",
    "- `transformers`, `datasets`, `torch`: LLM fine-tuning (e.g., DistilBERT)\n",
    "\n",
    "Kaggle environment:\n",
    "\n",
    "- PyTorch: 2.7.1+cu118\n",
    "- CUDA available: True\n",
    "- CUDA version: 11.8\n",
    "- GPU model: NVIDIA GeForce RTX 3050 Laptop GPU\n",
    "- Current device: 0\n",
    "\n",
    "Make sure all dependencies are installed before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dfec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU model: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Python executable: {sys.executable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e988c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from transformers import set_seed\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18f14d5",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "- Train set: `/kaggle/input/competitions/llm-classification-finetuning/train.csv`\n",
    "- Test set: `/kaggle/input/competitions/llm-classification-finetuning/test.csv`\n",
    "\n",
    "Train set columns:\n",
    "\n",
    "- `id, model_a, model_b, prompt, response_a, response_b, winner_model_a, winner_model_b, winner_tie`\n",
    "\n",
    "Test set columns:\n",
    "\n",
    "- `id, prompt, response_a, response_b`\n",
    "\n",
    "Goal: given the prompt and two responses, predict which response users prefer (A / B / tie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a3425",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/kaggle/input/competitions/llm-classification-finetuning\"\n",
    "train_path = os.path.join(data_root, \"train.csv\")\n",
    "test_path = os.path.join(data_root, \"test.csv\")\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "print(\"Train shape:\", df_train.shape)\n",
    "print(\"Test shape:\", df_test.shape)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afee0ef9",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Before training, visualize the dataset to understand its structure. Focus on:\n",
    "\n",
    "- Label distribution (winner)\n",
    "- Basic statistics and distributions of text lengths (prompt / response)\n",
    "- A few sample rows to understand the task format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148b9c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge winner columns into a single label\n",
    "def get_winner(row):\n",
    "    if row[\"winner_model_a\"] == 1:\n",
    "        return \"Model A\"\n",
    "    if row[\"winner_model_b\"] == 1:\n",
    "        return \"Model B\"\n",
    "    return \"Tie\"\n",
    "\n",
    "df_train[\"winner\"] = df_train.apply(get_winner, axis=1)\n",
    "\n",
    "df_train[\"winner\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eab2d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"len_prompt\"] = df_train[\"prompt\"].astype(str).apply(len)\n",
    "df_train[\"len_resp_a\"] = df_train[\"response_a\"].astype(str).apply(len)\n",
    "df_train[\"len_resp_b\"] = df_train[\"response_b\"].astype(str).apply(len)\n",
    "\n",
    "q_prompt = df_train[\"len_prompt\"].quantile(0.95)\n",
    "q_resp_a = df_train[\"len_resp_a\"].quantile(0.95)\n",
    "q_resp_b = df_train[\"len_resp_b\"].quantile(0.95)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "sns.histplot(df_train[\"len_prompt\"], bins=50, ax=axes[0])\n",
    "axes[0].set_title(\"Prompt Length (0-95% quantile)\")\n",
    "axes[0].set_xlim(0, q_prompt)\n",
    "\n",
    "sns.histplot(df_train[\"len_resp_a\"], bins=50, ax=axes[1])\n",
    "axes[1].set_title(\"Response A Length (0-95% quantile)\")\n",
    "axes[1].set_xlim(0, q_resp_a)\n",
    "\n",
    "sns.histplot(df_train[\"len_resp_b\"], bins=50, ax=axes[2])\n",
    "axes[2].set_title(\"Response B Length (0-95% quantile)\")\n",
    "axes[2].set_xlim(0, q_resp_b)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54368ac4",
   "metadata": {},
   "source": [
    "## 3. Model Choice and Principles\n",
    "\n",
    "This is a **text multi-class classification** task: given `(prompt, response_a, response_b)`, predict which response is preferred (or tie).\n",
    "\n",
    "This notebook uses **DistilBERT** as the base model:\n",
    "\n",
    "- DistilBERT is a distilled, smaller BERT with fewer parameters and faster inference, while preserving most semantic capability.\n",
    "- Pretraining learns general language representations; fine-tuning maps them to the preference classification task.\n",
    "- Model card and paper:\n",
    "  - https://huggingface.co/distilbert-base-uncased\n",
    "  - https://arxiv.org/abs/1910.01108\n",
    "\n",
    "### Input Construction Strategy\n",
    "\n",
    "We concatenate `(prompt, response_a, response_b)` into a single sequence so the model can compare both responses within one context window.\n",
    "\n",
    "```text\n",
    "Prompt: <prompt> \\n Response A: <response_a> \\n Response B: <response_b>\n",
    "```\n",
    "\n",
    "Why this works:\n",
    "- Self-attention aligns key information across segments, learning prompt-response alignment and A/B differences.\n",
    "- The classifier reads a single global representation (e.g., [CLS] or pooled embedding), effectively comparing all three segments.\n",
    "- A fixed template (Prompt / Response A / Response B) makes the input structure explicit and stable.\n",
    "\n",
    "The output layer is a 3-class classifier:\n",
    "- Class 0: Model A wins\n",
    "- Class 1: Model B wins\n",
    "- Class 2: Tie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab63fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_LOCAL_MODEL_DIR = \"/kaggle/input/models/boriszhangyyy/distilbert-finetuned-llm-pref-v2/pytorch/default/1/trained_model_v3\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "MODEL_NAME = HF_LOCAL_MODEL_DIR if os.path.isdir(HF_LOCAL_MODEL_DIR) else None\n",
    "if MODEL_NAME is None:\n",
    "    raise FileNotFoundError(\"No local model directory found at the Kaggle path.\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, local_files_only=True)\n",
    "\n",
    "label2id = {\"Model A\": 0, \"Model B\": 1, \"Tie\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "df_train[\"label\"] = df_train[\"winner\"].map(label2id)\n",
    "df_train[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642ca8a4",
   "metadata": {},
   "source": [
    "## 4. Metrics and Loss Function\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "- For multi-class classification, the standard choice is **cross-entropy loss**.\n",
    "- In `transformers`, `AutoModelForSequenceClassification` automatically applies cross-entropy when `labels` are provided.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "- **Accuracy**: correct predictions / total samples.\n",
    "- **Log Loss**: a Kaggle-standard metric that measures how close the predicted probability distribution is to the true labels (lower is better).\n",
    "\n",
    "In the Trainer, we compute both metrics via a custom `compute_metrics` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb6de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def normalize_text(x):\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    if isinstance(x, list):\n",
    "        return \" \".join([normalize_text(i) for i in x])\n",
    "    if isinstance(x, dict):\n",
    "        return \" \".join([f\"{k}:{normalize_text(v)}\" for k, v in x.items()])\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "            try:\n",
    "                v = json.loads(s)\n",
    "                return normalize_text(v)\n",
    "            except Exception:\n",
    "                pass\n",
    "        s = s.encode(\"utf-8\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "        s = s.replace(\"\\\\/\", \"/\")\n",
    "        return s\n",
    "    return str(x).encode(\"utf-8\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "def build_text(df):\n",
    "    prompt = df[\"prompt\"].apply(normalize_text)\n",
    "    resp_a = df[\"response_a\"].apply(normalize_text)\n",
    "    resp_b = df[\"response_b\"].apply(normalize_text)\n",
    "    model_a = df[\"model_a\"].apply(normalize_text) if \"model_a\" in df.columns else \"\"\n",
    "    model_b = df[\"model_b\"].apply(normalize_text) if \"model_b\" in df.columns else \"\"\n",
    "    return (\n",
    "        \"Model A: \" + model_a + \"\\n\" +\n",
    "        \"Model B: \" + model_b + \"\\n\" +\n",
    "        \"Prompt: \" + prompt + \"\\n\" +\n",
    "        \"Response A: \" + resp_a + \"\\n\" +\n",
    "        \"Response B: \" + resp_b\n",
    "    )\n",
    "\n",
    "def encode_texts(texts, batch_size=1024):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(\n",
    "            list(batch),\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=False,\n",
    "        )\n",
    "        input_ids.extend(enc[\"input_ids\"])\n",
    "        attention_mask.extend(enc[\"attention_mask\"])\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df_train,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=df_train[\"label\"],\n",
    ")\n",
    "\n",
    "train_text = build_text(train_df)\n",
    "val_text = build_text(val_df)\n",
    "test_text = build_text(df_test)\n",
    "\n",
    "train_enc = encode_texts(train_text)\n",
    "val_enc = encode_texts(val_text)\n",
    "test_enc = encode_texts(test_text)\n",
    "\n",
    "train_enc[\"labels\"] = train_df[\"label\"].tolist()\n",
    "val_enc[\"labels\"] = val_df[\"label\"].tolist()\n",
    "\n",
    "train_enc = Dataset.from_dict(train_enc)\n",
    "val_enc = Dataset.from_dict(val_enc)\n",
    "test_enc = Dataset.from_dict(test_enc)\n",
    "\n",
    "train_enc.set_format(\"torch\")\n",
    "val_enc.set_format(\"torch\")\n",
    "test_enc.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafeb82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    preds = probs.argmax(axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    try:\n",
    "        ll = log_loss(labels, probs)\n",
    "    except ValueError:\n",
    "        ll = float(\"nan\")\n",
    "    return {\"accuracy\": acc, \"log_loss\": ll}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a3c0cb",
   "metadata": {},
   "source": [
    "## 5. Model Building and Training\n",
    "\n",
    "We build a 3-class model with `AutoModelForSequenceClassification`:\n",
    "\n",
    "- `num_labels=3`\n",
    "- `id2label` / `label2id` map class IDs to readable labels.\n",
    "\n",
    "Key training parameters and what they do:\n",
    "- `learning_rate`: step size; too large causes instability, too small slows convergence. Typical range: 1e-5 to 5e-5.\n",
    "- `num_train_epochs`: number of full passes; higher can overfit. Monitor validation metrics as it increases.\n",
    "- `per_device_train_batch_size`: batch size per GPU; limited by VRAM. Use gradient accumulation to simulate larger batches.\n",
    "- `gradient_accumulation_steps`: accumulates gradients across steps; effective batch = batch_size Ã— accumulation_steps.\n",
    "- `weight_decay`: regularization to reduce overfitting; commonly 0.01.\n",
    "- `fp16`: mixed precision for faster training and lower memory usage on GPUs.\n",
    "- `eval_strategy` / `save_strategy`: evaluation and checkpointing cadence; must match when `load_best_model_at_end=True`.\n",
    "- `logging_steps`: log interval for tracking loss.\n",
    "- `save_total_limit`: limits checkpoint count to save disk space.\n",
    "- `warmup_ratio` or `warmup_steps`: warms up the learning rate for stability.\n",
    "\n",
    "Tuning tips:\n",
    "- Start with fewer epochs and a smaller batch to validate the pipeline, then scale up.\n",
    "- If validation loss rises, reduce epochs or increase regularization.\n",
    "- When VRAM is limited, use gradient accumulation instead of a larger batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524270e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    local_files_only=True,\n",
    ").to(device)\n",
    "\n",
    "inference_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=inference_args,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e45a1b",
   "metadata": {},
   "source": [
    "## 6. Validation Evaluation and Test Prediction\n",
    "\n",
    "1. Evaluate on the validation set and report accuracy and log_loss.\n",
    "2. Predict on the test set and generate `submission.csv` with:\n",
    "   - `id`\n",
    "   - `winner_model_a`, `winner_model_b`, `winner_tie` (predicted probabilities for each class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skip evaluation in inference-only mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e40041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test_enc)\n",
    "probs = torch.softmax(torch.tensor(predictions.predictions), dim=-1).numpy()\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": df_test[\"id\"],\n",
    "    \"winner_model_a\": probs[:, 0],\n",
    "    \"winner_model_b\": probs[:, 1],\n",
    "    \"winner_tie\": probs[:, 2],\n",
    "})\n",
    "\n",
    "submission_path = \"/kaggle/working/submission.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"Submission file saved to {submission_path}\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7507a9",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "- How to load and visualize Kaggle LLM Classification Finetuning `train.csv` / `test.csv` data\n",
    "- How to combine three text fields (prompt, response_a, response_b) into one model input\n",
    "- How to fine-tune a 3-class DistilBERT model with cross-entropy loss\n",
    "- How to evaluate on the validation set (accuracy, log_loss)\n",
    "- How to predict the test set and generate a submission file in the required format\n",
    "\n",
    "You can extend this by:\n",
    "\n",
    "- Trying a larger model or incorporating `model_a`/`model_b` as features\n",
    "- Improving the input construction (e.g., encode A/B separately then compare)\n",
    "- Adding richer visualizations and error analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
